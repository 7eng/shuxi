



## 数栖平台大数据基础组件技术文档

### 一、数栖平台存储技术

数栖平台采用HDFS作为底层主存储。

#### 1.HDFS

##### 1.1基本原理

##### 1.1.1 功能

Hadoop分布式文件系统（Hadoop Distributed File System）能提供高吞吐量的数据访问，适合大规模数据集方面的应用。

##### 1.1.2 结构

> 普通HA

HDFS包含主、备NameNode和多个DataNode。

HDFS是一个Master/Slave的结构，在Master上运行NameNode，而在每一个Slave上运行DataNode。

NameNode和DataNode之间的通信都是建立在TCP/IP的基础之上的。NameNode和DataNode都是设计成可以部署在运行Linux的服务器上。

> HA 模式


> 模块功能描述

| 角色                      | 描述                                       |
| ----------------------- | ---------------------------------------- |
| NameNode                | 用于管理文件系统的命名空间、目录结构、元数据信息以及提供备份机制等，分为：  <br><br>Active NameNode：管理文件系统的命名空间、维护文件系统的目录结构树以及元数据信息；记录写入的每个“数据块”与其归属文件的对应关系。 <br> <br>Standby NameNode：对Active NameNode进行监控；对Active NameNode中的数据进行备份；随时准备在Active NameNode出现异常时接管其服务。 |
| DataNode                | 用于存储每个文件的“数据块”数据，并且会周期性的向NameNode报告该DataNode的存放情况。 |
| DFSZKFailoverController | HA配置了一个zookeeper集群，用于ZKFC（DFSZKFailoverController）故障转移，当Active NameNode挂掉了，DFSZKFailoverController会自动切换Standby NameNode为standby状态 。 |
| JournalNode             | 主备NameNode之间通过一组JournalNode同步元数据信息，一条数据只要成功写入多数JournalNode即认为写入成功。通常配置奇数个JournalNode 。 |

##### 1.1.3原理

在HDFS内部，一个文件其实分成一个或多个“数据块”，这些“数据块”存储在DataNode集合里，NameNode负责保管和管理所有的HDFS元数据。客户端连接到NameNode，执行文件系统的“命名空间”操作，例如打开、关闭、重命名文件和目录，同时决定“数据块”到具体DataNode节点的映射。DataNode在NameNode的指挥下进行“数据块”的创建、删除和复制。另外客户端连接到DataNode，执行读写用户数据操作。



![HDFS](images/image-basic/1-HDFS.png)

##### 1.1.4 与其他组件的关系

- HDFS和HBase的配合关系

HBase是Apache的Hadoop项目的子项目，HBase利用Hadoop HDFS作为其文件存储系统。HBase位于结构化存储层，Hadoop HDFS为HBase提供了高可靠性的底层存储支持。HBase中的所有数据文件都可以存储在Hadoop HDFS文件系统上，除了HBase产生的一些日志文件。

- MapReduce和HDFS的配合关系

HDFS是hadoop 分布式文件系统，具有高容错性的特点，可以用来部署在低廉的硬件上，它提供了高吞吐量的特性，用来访问应用程序的数据，适合有超大数据集的应用程序。
而MapReduce是一种编程模型，用于大数据集（大于1TB）的并行运算。在MapReduce程序中计算的数据可以来自多个数据源，如local File、HDFS、数据库等。最常用的是HDFS，可以利用HDFS的高吞吐性能读取大规模的数据进行计算。同时在计算完成后，也可以将数据存储到HDFS。
对比Spark，MapReduce读取HDFS数据或者存储数据到HDFS中都相对简单。当MapReduce运行Task时，会基于用户编写的业务逻辑进行读取或存储数据。

- Spark和HDFS的配合关系

通常，Spark中计算的数据可以来自多个数据源，如local File、HDFS等。最常用的是HDFS，可以一次读取大规模的数据进行并行计算。在计算完成后，也可以将数据存储到HDFS。



##### 1.1.5 参考文档：

http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html

#### 2.Yarn

##### 2.1 基本原理

##### 2.1.1 功能

YARN是Hadoop 2.0中的资源管理系统，它是一个通用的资源模块，可以为各类应用程序进行资源管理和调度。YARN不仅仅局限于MapReduce一种框架使用，也可以供其他框架使用，比如Tez、Spark、Storm等。YARN类似于几年前的资源管理系统Mesos和更早的Torque。由于YARN的通用性，下一代的MapReduce核心已经从简单的支持单一应用的计算框架MapReduce转移到通用的资源管理系统YARN。



##### 2.2.2 结构

YARN模型主要由ResourceManager、ApplicationMaster和NodeManager组成
![YARN](images/image-basic/2-YARN.png)



| 角色                    | 描述                                       |
| --------------------- | ---------------------------------------- |
| Client                | YARN application客户端，可以通过客户端向ResourceManager提交任务，查询application运行状态等。 |
| ResourceManager(RM)   | 集群的资源管理器，基于应用程序对资源的需求进行调度。资源管理器提供一个调度策略的插件，它负责将集群资源分配给多个队列和应用程序。调度插件可以基于现有的能力调度和公平调度模型。 |
| NodeManager（NM）       | 负责执行应用程序的容器，同时监控应用程序的资源使用情况（CPU，内存，硬盘，网络）并且向ResourceManager汇报。 |
| ApplicationMaster(AM) | 即图中的App Mstr，负责相应的调度和协调，结合从ResourceManager获得的资源和NodeManager协同工作来运行和监控任务。 |
| Container             | 作为资源隔离，当前仅仅提供java虚拟机CPU、内存的隔离。           |



##### 原理

##### 与组件的关系


### 二、数栖平台离线计算及调度

 

#### 3.Hive

##### 3.1基本原理

##### 3.1.1 功能

##### 3.1.2 结构

##### 3.1.3 原理

##### 3.1.4 与组件的关系

 



#### Spark & Spark SQL

##### 基本原理

###### 功能

###### 结构

###### 原理

##### 与组件的关系

 





#### 三、数栖平台交换技术

 

#### DataX

##### 基本原理

###### 功能

###### 结构

###### 原理

##### 与组件的关系

 

#### 四、数栖平台实时计算

 

#### Flink

##### 基本原理

###### 功能

###### 结构

###### 原理

##### 与组件的关系



 

#### 三、数栖平台交换技术

 

#### 五、数栖平台即席计算

 

#### Presto

##### 基本原理

###### 功能

###### 结构

###### 原理

##### 与组件的关系

 

六、数栖平台性能优化文档

 

 

 

三、大数据技术生态圈

大数据如今已经不再是什么新的名词，五中全会大数据上升为国家战略，BAT 巨头早已布局多年，大数据时代已经真正来临，但我们真的准备好了么?

大家都知道大数据中蕴含大量的数据价值，比如说淘宝与天猫的用户消费行为、滴滴打车可以知道用户每天去了哪里、用户在优酷上都看了那些视频、移动运营商的海量客户终端信息以及上网行为、大型 零售商每天的销售数据、订餐网上用户每天吃了什么，等等大数据金矿无处不在。但淘出来的才是金子，否则只是一堆土而已，即占用场地，还要花钱去保管和维护这堆土。

大数据时代金矿已经有了，如何利用好这个金矿，某种意义上取决于我们手上的工具。熟话说“没有那金刚钻，就别揽瓷器活”，工具是否适用，直接决定着我们能否进行挖金，以及挖金的速度与效率。适合用铁锹还是挖掘机，对挖金来说有着质的不同。

大数据本身是个很宽泛的概念，Hadoop 生态圈(或者泛生态圈)基本上都是为了处理超过单机尺度的数据处理而诞生的。你可以把它比作一个厨房所以需要的各种工具，锅碗瓢盆，各有各的用处，互相之间又有重合。你可以用汤锅直接当碗吃饭喝汤，你可以用小刀或者刨子去皮。但是每个工具有自己的特性，虽然奇怪的组合也能工作，但是未必是最佳选择。

\1. HDFS

大数据，首先你要能存的下大数据。

传统的文件系统是单机的，不能横跨不同的机器。HDFS(Hadoop Distributed FileSystem)的设计本质上是 为了大量的数据能横跨成百上千台机器，但是你看到的是一个文件系统而不是很多文件系统。比如你说我要获取/hdfs/tmp/file1 的数据，你引用的是一个文件路径，但是实际的数据存放在很多不同的机器上。你作为用户，不需要知道这些，就好比在单机上你不关心文件分散在什么磁道什么扇区一样。HDFS 为你管理这些数据。

16

第一章基础知识

 2.MapReduce

存的下数据之后，你就开始考虑怎么处理数据。虽然 HDFS 可以为你整体管理不同机器上的数据，但是这些数据太大了。一台机器读取成 T 上 P 的数据(很大的数据哦，比如整个东京热有史以来所有高清电影的大小甚至更大)，一台机器慢慢跑也许需要好几天甚至好几周。对于很多公司来说，单机处理是不可忍受的，比如微博要更新 24 小时热博，它必须在 24 小时之内跑完这些处理。那么我如果要用很多台机器处理，我就面临了如何分配工作，如果一台机器挂了如何重新启动相应的任务，机器之间如何互相通信交换数据以完成复杂的计算等等。这就是 MapReduce / Tez / Spark 的功能，MapReduce 是第一代计算引擎， Tez和 Spark 是第二代。MapReduce 的设计，采用了很简化的计算模型，只有 Map 和 Reduce 两个计算过程 (中间用 Shuffle 串联)，使用这个模型，已经可以处理大数据领域很大一部分问题了。

那什么是 Map，什么是 Reduce?

考虑如果你要统计一个巨大的文本文件(存储在类似 HDFS 上)，你想要知道这个文本里各个词的出现频率。你启动了一个 MapReduce 程序。Map 阶段，几百台机器同时读取这个文件的各个部分，分别把各自读到的部分分别统计出词频，产生类似(hello, 12100 次)，(world，15214 次)等等这样的 Pair(这里把 Map 和 Combine 放在一起说以便简化);这几百台机器各自都产生了如上的集合，然后又有几百台机器启动 Reduce 处理。Reducer 机器 A 将从 Mapper 机器收到所有以 A 开头的统计结果，机器 B 将收到 B 开头的词汇统计结果(当然实际上不会真的以字母开头做依据，而是用函数产生 Hash 值以避免数据串化。因为类似 X 开头的词肯定比其他要少得多，而你不希望数据处理各个机器的工作量相差悬殊)。然后这些 Reducer 将再次汇总，如(hello，12100)+(hello，12311)+(hello，345881)= (hello，370292)。每个 Reducer 都如上处理，你就得到了整个文件的词频结果。

这看似是个很简单的模型，但很多算法都可以用这个模型述了。

Map+Reduce的简单模型很直接很暴力，虽然好用，但是很笨重。第二代的Tez和Spark除了内存Cache 之类的新 feature，本质上来说，是让 Map/Reduce 模型更通用，让 Map 和 Reduce 之间的界限更模糊，数据交换更灵活，更少的磁盘读写，以便更方便地述复杂算法，取得更高的吞吐量。

\3. Hive

有了 MapReduce、Tez 和 Spark 之后，程序员发现，MapReduce 的程序写起来真麻烦，希望能简化这个过程。这就好比你有了汇编语言，虽然你几乎什么都能干了，但是你还是觉得繁琐，希望有个更高层更抽象的语言层来 述算法和数据处理流程。于是就有了 Pig 和 Hive，Pig 是接近脚本方式去述 MapReduce， Hive 则用的是 SQL。它们把脚本和 SQL 语言翻译成 MapReduce 程序，丢给计算引擎去计算，而你就从繁琐的 MapReduce 程序中解脱出来，用更简单更直观的语言去写程序了。

有了 Hive 之后，人们发现 SQL 对比 Java 有巨大的优势。一个是它太容易写了，刚才词频的东西，用 SQL  述就只有一两行，而MapReduce 写起来大约要几十上百行。更重要的是，非计算机背景的用户终于感受到了爱:我也会写 SQL!于是数据分析人员终于从乞求工程师帮忙的窘境解脱出来，工程师也从写奇怪的一次性的处理程序中解脱出来，大家都开心了。Hive 逐渐成长成了大数据仓库的核心组件。甚至很多公司的流水线作业集完全是用 SQL  述，因为易写易改，一看就懂，容易维护。

17

第一章基础知识

 4.Impala，Presto，Drill

自从数据分析人员开始用 Hive 分析数据之后，它们发现 Hive 在 MapReduce 上跑，慢如流水!流水线作业集也许没啥关系，比如 24 小时更新的推荐，反正 24 小时内跑完就算了。但是数据分析时，人们总是希望能跑更快一些。比如我希望看过去一个小时内多少人在天线宝宝页面驻足，分别停留了多久，对于一个巨型网站的海量数据，这个处理过程也许要花几十分钟甚至很多小时。而这个分析也许只是万里长征的第一步，你还要看多少人浏览了游戏，多少人看了拉赫曼尼诺夫的 CD，以便跟老板汇报，我们的用户是宅男更多还是文艺青年/少女更多。你无法忍受等待的折磨，只能跟帅帅的工程师蝈蝈说，快，快，再快一点!

于是 Impala，Presto，Drill 诞生了(当然还有无数非著名的交互 SQL 引擎，就不一一列举了)。三个系统的核心理念是，MapReduce 引擎太慢，因为它太通用、太强壮、太保守，我们 SQL 需要更轻量、更激进地获取资源、更专门地对 SQL 做优化，而且不需要那么多容错性保证(因为系统出错了大不了重新启动任务，如果整个处理时间更短的话，比如几分钟之内)。这些系统让用户更快速地处理 SQL 任务，牺牲了通用性、稳定性等特性。如果说 MapReduce 是大砍刀，砍啥都不怕，那上面三个就是剔骨刀，灵巧锋利，但是不能搞太大太硬的东西。

\5. Spark

这些系统，说实话，一直没有达到人们期望的流行度。因为这时候又两个异类被造出来了，他们是 Hive on Tez / Spark 和 SparkSQL。它们的设计理念是，MapReduce 慢，但是如果我用新一代通用计算引擎 Tez 或者 Spark 来跑 SQL，那我就能跑的更快，而且用户不需要维护两套系统。这就好比如果你厨房小，人又懒，对吃的精细程度要求有限，那你可以买个电饭煲，能蒸能煲能烧，省了好多厨具。

\6. Storm

上面的介绍，基本就是一个数据仓库的构架了。底层 HDFS，上面跑 MapReduce/Tez/Spark，再在上面跑 Hive、Pig。或者 HDFS 上直接跑 Impala，Drill，Presto。这解决了中低速数据处理的要求。

  那如果我要更高速的处理呢?

如果我是一个类似微博的公司，我希望显示不只是 24 小时热博，我想看一个不断变化的热播榜，更新延迟在一分钟之内，上面的手段都将无法胜任。于是又一种计算模型被开发出来，这就是 Streaming(流) 计算。Storm 是最流行的流计算平台。流计算的思路是，如果要达到更实时的更新，我何不在数据流进来的时候就处理了?比如还是词频统计的例子，我的数据流是一个一个的词，我就让他们一边流过我就一边开始统计了。流计算很高明，基本无延迟，但是它的短处是不灵活，你想要统计的东西必须预先知道，毕竟数据流过就没了，你没算的东西就无法补算了。虽然它是个很好的东西，但是无法替代上面数据仓库和批处理系统。

18

第一章基础知识

 7.Cassandra，HBase，MongoDB

还有一个有些独立的模块是 KV Store，比如 Cassandra、HBase、MongoDB 以及很多很多很多很多其他的(多到无法想象)。KV Store 就是说，由于我有一堆键值(key)，我能很快速滴获取与这个 Key 绑定的数据。比如我用身份证号就能取到你的身份数据。这个动作用 MapReduce 也能完成，但是很可能要扫整个 数据集。而 KVStore 专用来处理这个操作，所有存和取都专门为此优化了。从几个 P 的数据中查找一个身份证号，也许只要零点几秒。这让大数据公司的一些专门操作被大大优化了。比如我网页上有个根据订单号查找订单内容的页面，而整个网站的订单数量无法单机数据库存储，我就会考虑用 KV Store 来存。KV Store 的理念是，基本无法处理复杂的计算，大多没法 JOIN，也许没法聚合，没有强一致性保证(不同数据分布在不同机器上，你每次读取也许会读到不同的结果，也无法处理类似银行转账那样的强一致性要求的操作)，但是就是快、极快。

每个不同的 KV Store 设计都有不同取舍，有些更快，有些容量更高，有些可以支持更复杂的操作。必有一款适合你。

\8. YDB

YDB 是延云针对用户对大数据探索式、即席分析的需求而开发的分析软件，可以说是笔者的心头好。

YDB 将传统数据库索引技术应用在大数据技术上，打破目前大数据计算技术的僵局。将大数据检索向时效性更强，查询方式更灵活，执行效率更高的方向演进。虽然引用传统索引技术，但是对硬件的需求并不比 Hadoop 高，不会让小型用户望而却步。技术上 YDB 采用Java 语言编写，接地气，SQL 接口，用户也更易于上手使用，同时每天千亿增量万亿总量的数据量也能满足高端用户的需求。YDB 主要技术方向在大索引，大索引的好处在于加快了检索的速度，减少查询中的分组、统计和排序时间，通过高系统的性能 和响应时间来节约资源。大索引技术的运用才能使 YDB 在如此大规模的数据量下依然保持查询响应时间在几秒，数据导入延迟在几分钟。

大数据时代拼的不仅仅是数据量有多大，还要拼速度，拼谁的更快、更准、成本更低。大数据的运用领域还在不断的扩张，大索引技术还有很长的路要走。终有一天大数据会带给我们震撼世界的影响。

四、大数据分析与处理 1. 文件批处理

以 MapReduce、Hive 为典型代表，批处理模式解决了传统的数据仓库无法处理海量数据的难题。通过批处理计算引擎，使得海量数据分析成为可能。没有批处理引擎的诞生，也就没有今天风风火火的大数据。数据通常积累达到一个周期后定期运行，也就是所谓的 T+1 数据，即典型的 T 为一天，即数据延迟一

天。

批处理的业务通常一次可以计算很大量的数据，但对计算的时效性要求不高，通常来说一个 HiveSQL

可以轻松处理几 T 的数据，运行时间从几分钟到几小时不等，如果是百亿规模的数据分析时间可能会达到

19

数个小时。

第一章基础知识

    2. 内存批处理

以 Spark 与 Impala 为典型代表，内存批处理与基于文件批处理很类似，只不过由于数据的处理过程中数据放在内存里(甚至原始数据也在内存里)，由于内存的读写速度远远高于磁盘的读写速度，所以一般内存批处理系统的查询计算速度远远高于文件批处理系统的计算速度。

但是内存系统的缺点也是不言而喻的，内存在当今的硬件时代还是比较昂贵，而大数据领域的数据又都是比较庞大的，所以成本还是比较高昂的。

\3. 流计算

全量数据处理使用的大多是鼎鼎大名的 Hadoop 或者 Hive，作为一个批处理系统，hadoop 以其吞吐量大、自动容错等优点，在海量数据处理上得到了广泛的使用。但是，Hadoop 不擅长实时计算，因为它天然就是为批处理而生的，这也就是流计算系统(实时处理系统)诞生的意义，实时系统以 Storm 与 SparkStreaming为代表。Apache Storm最为知名，阿里也在Storm的基础上重新用java重写了Storm，命名为 Jstorm，并且又重新贡献了给 Apache 社区。

流计算系统的特点

低延迟。既然是是实时计算系统了，延迟是一定要低的。时效性非常好，一般采用 Kafka 消息队列的方式导入，时效性可达几秒可见。

高性能。指标预计算:预先将需要查询的数据计算好，查询的时候直接使用预计算好的结果，性能非常高。

分布式。系统都是为应用场景而生的，如果你的应用场景、你的数据和计算单机就能搞定，那么不用考虑这些复杂的问题了。大数据所说的是单机搞不定的情况。

可扩展。伴随着业务的发展，我们的数据量、计算量可能会越来越大，所以希望这个系统是可扩展的。

容错。这是分布式系统中通用问题。一个节点挂了不能影响我的应用。缺点:

无法查看明细数据: 只能看特定粒度的汇总结果，而过车记录是无法先计算出来的，即无法预知那个车有可能会犯罪，那个车会出事故，故无法预计算。

\4. 预计算分析

全量数据处理系统，存在的主要问题就是查询性能太差，也无并发性而言。为了解决查询延迟问题，很多离线系统的做法就是预先将每天要分析统计的指标计算好，存储在一个可以高速访问的系统里面如 HBase 或者传统数据里面，供报表系统进行展示，供常规多维分析使用。

随后发现这类需求有一共性，企业针对每种业务都单独写一遍 Hive SQL ,再导入到传统数据库里面，再供报表系统查询。很麻烦，而且这类的需求很多，所以业界出现了很多预计算系统，主要目的就是将业务进行预先计算，供业务进行访问，主要特点是使用非常便捷，极大的缩短的程序开发的时间，升了开发 效率，有的甚至将离线计算与流计算进行了结合， 供了更加实时的报表功能。

业界典型的产品代表，莫过于 Apache Kylin。Kylin 是为减少在 Hadoop 上百亿规模数据查询延迟而设 20

第一章基础知识

 计

 Hadoop ANSI SQL 接口:

Kylin 为 Hadoop  供标准 SQL 支持大部分查询功能   交互式查询能力:

通过 Kylin，用户可以与 Hadoop 数据进行亚秒级交互，在同样的数据集上供比 Hive 更好的性能   多维立方体(MOLAPCube):

用户能够在 Kylin 里为百亿以上数据集定义数据模型并构建立方体   与BI工具无缝整合:

Kylin 供与 BI 工具，如 Tableau，的整合能力，即将供对其他工具的整合

21

第一章基础知识

    5. 即席分析

预计算系统可以有效的解决数据查询的响应时间问题，但是现实中有很多数据是无法实现预计算的，或者预计算的代价是非常昂贵的，一个几万列的大宽表，各种维度笛卡尔组合后的结果集甚至比原生数据都多好多倍，如果用户在来个模糊检索，预计算的指标值多的简直是不可想象的。只有那些预先知道的场景可以使用预计算，有些场景是无法预先知道的，也就无法进行预计算的。

即席(Ad Hoc)查询与分析是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。

在一个即席分析系统里面，用户的查询条件不再像预计算系统那样受限，检索、统计、排序等都根据用户的意愿去查询，查询的列数也不受任何限制，可以是一个维度也可以是任意维度的组合。

“即席分析”源于互联网公司对海量数据的即时性分析，后台系统和数据分析师通过不断地对海量数据进行探索性的查询与分析，挖掘大数据潜在价值，是互联网公司将数据变现的重要手段。

随着大数据在各行各业的应用，越来越多的行业客户对即席分析有着强烈的需求，要求能够对千亿甚至万亿规模数据进行高时效性地分析挖掘，这也是衡量各行业大数据应用水平的关键尺度。

事实上，我们已经看到，即席分析必将成为大数据生态中的最为典型的需求场景之一，而延云的目标就是成为大数据即席分析领域的标准。

一个典型的即席分析系统应该具备如下特征

\1. 数据实时导入，秒级可见

\2. 任意维度组合的多维分析，维度组合不受限。

\3. 即席查询:想查什么就查什么，秒级响应，不应该受束缚。 4. 模糊检索:可以像百度那样快速的搜索，匹配。

\6. 探索性、验证型分析

当我们对大数据以及大数据分析完全没有头绪，当我们拿到数据应该怎么做呢，如果不知道怎么做，那就先进行探索性分析吧。

探索性分析，实现我们并不知道需要查什么?那就是探索性先查一下，看到数据后，有可能会激发下一步的想法，再进一步的查询，直到分析出问题所在。

探索性分析最直观的场景的就是通过日志分析 BUG，一开始我们并不知道 BUG 在什么地方，而是先搜索下日志，了解下程序运行的一个概况，可能会意外的发现某个节点有异常，然后在深入的了解这个有异常的节点的日志，直到追查到 BUG 所在。

探索性分析在公安破案检索场景也是十分有效的，很多时候公安行业破一个案子，但是并不知道谁是嫌疑人，那么可能就会先搜索出与案件相关的时间、地点、人物等进行碰撞，如果碰撞到一些有价值的线索，就会在碰撞的结果上进一步追踪，根据各种线索与规律匹配到犯罪嫌疑人。

设想一个使用场景,我们的美女数据分析师，她有一个新的想法要验证。要验证她的想法，需要在一个上亿条数据上面，跑一个查询，看看结果和她的想法是不是一样，她可不希望等太长时间，最好几秒钟结果就出来。当然她的想法不一定完善，还需要不断调整条件。然后她验证了想法，发现了数据中的价值。最后，她可以将这个语句完善成一个长期运行的任务。